{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1JVIgc3vh4uO0RfmUFTHO5AoRLIMobfDI","timestamp":1664782859239},{"file_id":"1lFTqa8be4v9S0fttU5eFTGjtcXnaCnST","timestamp":1664779023291}],"collapsed_sections":[],"mount_file_id":"1JVIgc3vh4uO0RfmUFTHO5AoRLIMobfDI","authorship_tag":"ABX9TyOlEUWT06bfawG2WZJ6xYEI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tZWlkb6WYZbH","executionInfo":{"status":"ok","timestamp":1664776026142,"user_tz":-540,"elapsed":19064,"user":{"displayName":"Hyeri Cho","userId":"04168557094514846908"}},"outputId":"58dd14a8-12e2-415b-b265-a399a6593020"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# NLP(자연어 처리)\n","- 자연어 :  일상 생활에서 사용하는 보편적인 언어\n","- 자연어 처리 : 컴퓨터가 자연어 처리하는 일\n"],"metadata":{"id":"qd1FwxAuYuhY"}},{"cell_type":"markdown","source":["# NLP 처리 대표 4가지\n","### 1.음성 인식(Speech Recognition)\n","### 2.번역(Translation)\n","### 3.요약(Text Summary)\n","### 4.분류(Text Classification)\n","\n","<예시>\n","- Chatbot\n","- SIRI\n","- Translator\n"],"metadata":{"id":"8xiZ02CblFUD"}},{"cell_type":"markdown","source":["# 텍스트 전처리\n","- 컴퓨터에서 자연어를 효과적으로 처리할 수 있도록 전처리 과정 \n","\n","- sentence - Tokenization - Cleaning,Stemming - Encoding - Sorting - Padding, Similarity\n"],"metadata":{"id":"yehyXOB3ZMjB"}},{"cell_type":"markdown","source":["## 1. 토큰화 \n","\n","### 단어 토큰화\n","- 주어진 문장에서 의미부여가 가능한 단위 찾기\n","\n","### 문장 토큰화\n","- 문장 단위로 전체 문단에서 토큰화\n","\n","### ** 한국어 토큰화\n","- 활용형이 많고 품사가 많아 어려움  "],"metadata":{"id":"-WX33E-cZa9z"}},{"cell_type":"markdown","source":["1) 단락을 문장단위로 분리해본다\n","- sent_tokenize 이용"],"metadata":{"id":"eHh-UYnEbHgA"}},{"cell_type":"code","source":["from nltk.tokenize import sent_tokenize\n","nltk.download('punkt')\n","para = \"Hello World. It's good to see you. Thanks for buying this book.\"\n","print(sent_tokenize(para))\n","['Hello World.', \"It's good to see you.\", 'Thanks for buying this book.']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"74UmBoKgbGue","executionInfo":{"status":"ok","timestamp":1664777492398,"user_tz":-540,"elapsed":416,"user":{"displayName":"Hyeri Cho","userId":"04168557094514846908"}},"outputId":"fd61e9b7-f5d6-4507-9792-a543317aae02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello World.', \"It's good to see you.\", 'Thanks for buying this book.']\n"]},{"output_type":"execute_result","data":{"text/plain":["['Hello World.', \"It's good to see you.\", 'Thanks for buying this book.']"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["from nltk.tokenize import TreebankWordTokenizer\n","tokenizer = TreebankWordTokenizer()\n","text = \"NLP is natural language processing.\"\n","print(tokenizer.tokenize(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rYgDrW33ZZo_","executionInfo":{"status":"ok","timestamp":1664776410734,"user_tz":-540,"elapsed":16,"user":{"displayName":"Hyeri Cho","userId":"04168557094514846908"}},"outputId":"f4a5b8c7-5586-4a3c-c099-c851be340a42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['NLP', 'is', 'natural', 'language', 'processing', '.']\n"]}]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","print(word_tokenize(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v1YGOSxOZWHc","executionInfo":{"status":"ok","timestamp":1664777517272,"user_tz":-540,"elapsed":544,"user":{"displayName":"Hyeri Cho","userId":"04168557094514846908"}},"outputId":"0b0acc39-c845-4423-9191-aa0912bfdaa2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['NLP', 'is', 'natural', 'language', 'processing', '.']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["### 2. 정제(Cleaning)\n","- 데이터 사용 목적에 따라 노이즈 제거\n","   - 1)대문자 vs 소문자\n","   - 2) 출현횟수 적은 단어 제거 \n","   - 3) 길이가 짧은 단어, 지시대명사, 관사 제거"],"metadata":{"id":"br0r2N18mRmF"}},{"cell_type":"markdown","source":["### 3.추출(Stemming)\n","- 어간(Stem) : 단어의 의미를 담은 핵심\n","- 접사(Affix) : 단어에 추가 용법 부여\n","- 어간 추출(Stemming) : 품사정보 X \n","- 표제어 추출(Lemmatization) : 품사 정보 O\n","- 단어의 형태소 level에서 분석을 하게 되면 다른 품사 또는 다른 시제의 단어도 같은 형태로 토큰화 가능 \n","\n","** Poter Algorithm : 대표적인 Stemming 방법 \n","\n"],"metadata":{"id":"zQoEHn_9dKT9"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer, LancasterStemmer\n","stem1 = PorterStemmer()\n","stem2 = LancasterStemmer()\n","words = [\"eat\", \"ate\", \"eaten\", \"eating\"]\n","print(\"Porter Stemmer :\", [stem1.stem(w) for w in words])\n","print(\"Lancaster Stemmer :\", [stem2.stem(w) for w in words])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9OhagXZbwR-6","executionInfo":{"status":"ok","timestamp":1664782271183,"user_tz":-540,"elapsed":1151,"user":{"displayName":"Hyeri Cho","userId":"04168557094514846908"}},"outputId":"553c3fbb-42b1-4f1e-b696-5e46100c143d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Porter Stemmer : ['eat', 'ate', 'eaten', 'eat']\n","Lancaster Stemmer : ['eat', 'at', 'eat', 'eat']\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('omw-1.4')\n","from nltk import WordNetLemmatizer\n","nltk.download('wordnet')\n","lemm = WordNetLemmatizer()\n","words = [\"eat\", \"ate\", \"eaten\", \"eating\"]\n","print(\"WordNet Lemmatizer :\",[lemm.lemmatize(w, pos=\"v\") for w in words]) \n","# 이 단어들이 모두 동사라고 지정을 해줬음\n","# 시제나 형태가 달라도 모두 eat 이라는 결과를 확인 "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IJsZiWNzwfMP","executionInfo":{"status":"ok","timestamp":1664782341223,"user_tz":-540,"elapsed":2345,"user":{"displayName":"Hyeri Cho","userId":"04168557094514846908"}},"outputId":"0810479d-71b7-4b20-bc97-bf4ca0096357"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["WordNet Lemmatizer : ['eat', 'eat', 'eat', 'eat']\n"]}]},{"cell_type":"markdown","source":["### 4.불용어(Stopword)\n","- 문장에서 대세로 작용하지 않는, 중요도가 낮은 단어 제거 \n","\n","<제거 방법>\n","- 1) 불용어 목록 받아오기\n","- 2) 정제할 문장 토큰화 하기\n","- 3) 토큰화된 각 단어마다 \n","  - 단어가 불용어 목록에 없는 경우 - 정제 결과에 추가\n","  - 단어가 불용어 목록에 있는 경우 - pass "],"metadata":{"id":"ZJmJW3vPdhdz"}},{"cell_type":"code","source":["example = \"Family is not an important thing. It's everything.\"\n","stop_words = set(stopwords.words('english')) # 영어에서의 불용어 \n","\n","print('영어 불용어 갯수:',len(nltk.corpus.stopwords.words('english')))\n","print(nltk.corpus.stopwords.words('english')[:40])\n","print('------------------------------------------------------------------')\n","\n","word_tokens = word_tokenize(example)\n","\n","result = []\n","for token in word_tokens: \n","    if token not in stop_words: \n","        result.append(token) \n","\n","print('토큰화 : ',word_tokens) \n","print('불용어 제거 : ',result) \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MYHatTCvYkYe","executionInfo":{"status":"ok","timestamp":1664778050615,"user_tz":-540,"elapsed":20,"user":{"displayName":"Hyeri Cho","userId":"04168557094514846908"}},"outputId":"c67be716-8aed-4872-e410-11ed45ddc9f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["영어 불용어 갯수: 179\n","['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this']\n","------------------------------------------------------------------\n","토큰화 :  ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n","불용어 제거 :  ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"GIOrkf4OmOfk"}},{"cell_type":"markdown","source":["### 5.정수 인코딩 및 Sorting()\n","- 처음 보는 모든 단어에 정수 부여\n","- 등장 횟수가 많은 단어에 앞번호 부여"],"metadata":{"id":"l51pUwNkgp4G"}},{"cell_type":"markdown","source":["Python enumerate 연산 "],"metadata":{"id":"YcV4OYAUnrFq"}},{"cell_type":"code","source":["mylist = ['English','Math','Science']\n","for n, name in enumerate(mylist):\n","    print(\"Course : {}, Number : {}\".format(name,n))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8EC88UYgYk3j","executionInfo":{"status":"ok","timestamp":1664778537189,"user_tz":-540,"elapsed":405,"user":{"displayName":"Hyeri Cho","userId":"04168557094514846908"}},"outputId":"35e9b289-6634-4840-f16d-5f418be55a5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Course : English, Number : 0\n","Course : Math, Number : 1\n","Course : Science, Number : 2\n"]}]},{"cell_type":"markdown","source":["  - 정수 인코딩 및 High-Frequency Sorting"],"metadata":{"id":"ey4KnxkdxTWz"}},{"cell_type":"code","source":["vocab = {'apple' : 2, 'July' : 6, 'piano' : 4, 'cup' : 8, 'orange' : 1} # Bag of words (bow) 의 결과물 : 단어의 빈도수\n","vocab_sort = sorted(vocab.items(), key= lambda x:x[1], reverse= True)\n","print(vocab_sort)\n","word2inx = {word[0] : index + 1 for index, word in enumerate(vocab_sort)}\n","print(word2inx)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qcmx2uyZxSMf","executionInfo":{"status":"ok","timestamp":1664782555879,"user_tz":-540,"elapsed":354,"user":{"displayName":"Hyeri Cho","userId":"04168557094514846908"}},"outputId":"26510ec2-f93a-4565-af15-49307667ae22"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[('cup', 8), ('July', 6), ('piano', 4), ('apple', 2), ('orange', 1)]\n","{'cup': 1, 'July': 2, 'piano': 3, 'apple': 4, 'orange': 5}\n"]}]},{"cell_type":"markdown","source":["  - BoW로 만들어진 토큰화의 결과를 가장 높은 빈도수부터 재정렬하고, 이를 통해 정수 인코딩을 하는 과정\n","\n","- word2inx = {}를 만들고, 리스트에 없는 단어의 경우 새로 리스트와 BoW에 단어를 추가하고, 리스트에 있는 단어는 inx + 1"],"metadata":{"id":"0tXT2E9Wxezt"}},{"cell_type":"code","source":["from nltk.tokenize import TreebankWordTokenizer\n","tokenizer = TreebankWordTokenizer()\n","text = \"Model-based RL don't need a value fuction for the policy. \" \\\n","    \"but some of Model-based RL algorithms do have a value function. \"\n","token_text = tokenizer.tokenize(text)\n","word2inx = {}\n","Bow = []\n","for word in token_text:\n","    if word not in word2inx.keys():\n","        word2inx[word] = len(word2inx)\n","        Bow.insert(len(word2inx)-1,1)\n","    else:\n","        inx = word2inx.get(word)\n","        Bow[inx] += 1\n","print(word2inx)\n","print(Bow)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ilAJOAyRxeTp","executionInfo":{"status":"ok","timestamp":1664782584330,"user_tz":-540,"elapsed":281,"user":{"displayName":"Hyeri Cho","userId":"04168557094514846908"}},"outputId":"02ca7c68-e545-49f8-bb4c-25baeaec4ac6"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["{'Model-based': 0, 'RL': 1, 'do': 2, \"n't\": 3, 'need': 4, 'a': 5, 'value': 6, 'fuction': 7, 'for': 8, 'the': 9, 'policy.': 10, 'but': 11, 'some': 12, 'of': 13, 'algorithms': 14, 'have': 15, 'function': 16, '.': 17}\n","[2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"]}]},{"cell_type":"markdown","source":["### 6.Padding(Zero-padding)\n","- 문장의 길이가 가장 긴 값을 key\n","- 문장들에 정수 인코딩을 거친다.\n","- key의 길이에 맞춰서 각 문장에 0을 넣어 문장의 길이를 맞춤\n","\n","\n","    - One-hot Encoding\n","      : 저장공간이 많이 든다. - 평소에는 정수형, 필요할때만 원핫 인코딩으로 바꿔준다.\n","\n","\n","    - Word2vec Encoding\n","      : 단어의 유사성을 인코딩에 반영, 인코딩 벡터가 비슷하다 = 단어가 유사하다.\n","\n","\n","    - TF-IDF(Term Frequency - Inverse Document Frequency)\n","      : 단어들의 중요한 정도를 가중치로 매김\n","\n","  \n"],"metadata":{"id":"RApMYJaroa6Q"}}]}